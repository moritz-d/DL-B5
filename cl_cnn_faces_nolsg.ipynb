{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification of Faces\n",
    "\n",
    "In this excercise, we try to classify images of : \"Miranda_Cosgrove\" \"Chris_Martin\" \"Emma_Stone\" \"Jamie_Foxx\" \"Steve_Jobs\" \"Zac_Efron\" \"Sandra_Oh\" \"Taryn_Manning\". The data is from a random sample of 8 persons of the OXFORD VGG Face dataset, more information here: http://www.robots.ox.ac.uk/~vgg/data/vgg_face/."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading of Data\n",
    "\n",
    "You can download the data using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Der Befehl \"ls\" ist entweder falsch geschrieben oder\n",
      "konnte nicht gefunden werden.\n"
     ]
    }
   ],
   "source": [
    "# Downloading the data, if it does not exist, this will take some time... (>120MB)\n",
    "import urllib\n",
    "import os\n",
    "if not os.path.isfile('8_faces_train_and_valid.hdf5'):\n",
    "    urllib.request.urlretrieve(\"https://www.dropbox.com/s/dx03vk6dmbtoywn/8_faces_train_and_valid.hdf5?dl=1\",\"8_faces_train_and_valid.hdf5\")\n",
    "!ls -l 8_faces_train_and_valid.hdf5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['X_train_8_faces', 'X_valid_8_faces', 'Y_train_8_faces', 'Y_valid_8_faces']\n",
      "(2000, 48, 48, 3)\n",
      "(2000,)\n",
      "(400, 48, 48, 3)\n",
      "(400,) float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-cb1dfbb9d158>:4: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "  X_train = h5f_X['X_train_8_faces'].value\n",
      "<ipython-input-3-cb1dfbb9d158>:6: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "  Y_train = h5f_X['Y_train_8_faces'].value\n",
      "<ipython-input-3-cb1dfbb9d158>:8: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "  X_valid = h5f_X['X_valid_8_faces'].value\n",
      "<ipython-input-3-cb1dfbb9d158>:10: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "  Y_valid = h5f_X['Y_valid_8_faces'].value\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "h5f_X = h5py.File('8_faces_train_and_valid.hdf5', 'r')\n",
    "print(list(h5f_X.keys()))\n",
    "X_train = h5f_X['X_train_8_faces'].value\n",
    "print(X_train.shape)\n",
    "Y_train = h5f_X['Y_train_8_faces'].value\n",
    "print(Y_train.shape)\n",
    "X_valid = h5f_X['X_valid_8_faces'].value\n",
    "print(X_valid.shape)\n",
    "Y_valid = h5f_X['Y_valid_8_faces'].value\n",
    "print(Y_valid.shape, X_valid.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1a3dd97eee0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n = 0 #Miranda Cosgrove\n",
    "n = 1 #Chris Martin (singer)\n",
    "n = 2 #Emma Stone (actress)\n",
    "n = 3 #Jamie Foxx (actor)\n",
    "n = 4 #Steve Jobs (entrepreneur)\n",
    "n = 5 #Zac Efron (actor)\n",
    "n = 6 #Sandra Oh (actress)\n",
    "n = 7 #Taryn Manning (actress)\n",
    "n = 4 \n",
    "idx=np.reshape(np.where(Y_train[0:len(Y_train)]==n),(250))\n",
    "print(len(Y_train[idx]))\n",
    "plt.imshow(np.asarray(X_train[idx][0],dtype=\"uint8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['X_test_8_faces', 'Y_test_8_faces']\n",
      "(400, 48, 48, 3)\n",
      "(400,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-fa69679b5ec8>:6: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "  X_test = h5f_X['X_test_8_faces'].value\n",
      "<ipython-input-5-fa69679b5ec8>:8: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "  Y_test = h5f_X['Y_test_8_faces'].value\n"
     ]
    }
   ],
   "source": [
    "#Downloading the data, if it does not exist\n",
    "if not os.path.isfile('8_faces_test.hdf5'):\n",
    "  urllib.request.urlretrieve(\"https://www.dropbox.com/s/ugxrdo0lpc2ixvr/8_faces_test.hdf5?dl=1\",\"8_faces_test.hdf5\")\n",
    "h5f_X = h5py.File('8_faces_test.hdf5', 'r')\n",
    "print(list(h5f_X.keys()))\n",
    "X_test = h5f_X['X_test_8_faces'].value\n",
    "print(X_test.shape)\n",
    "Y_test = h5f_X['Y_test_8_faces'].value\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(X_train),np.max(X_train),np.min(X_test),np.max(X_test)\n",
    "X_train /= 255\n",
    "X_valid /= 255\n",
    "X_test /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a) Training an CNN from scratch\n",
    "\n",
    "Train a model (as shown below) from scratch, using the training data for training and the validation data for validation. For all activations in the hidden layer use the 'ReLU', for the last layer use softmax. Use 3x3 convolutions and use padding = same.\n",
    "\n",
    "* Plot the learning curves: (epochs vs training loss and validation loss) and epochs vs accuracy. \n",
    "* Calculate the accuracy on the test set (you should reach an accuracy of about 0.54)\n",
    "* Calculate the confusion matrix\n",
    "* Have a look at missclassified examples\n",
    "\n",
    "Image of the network:\n",
    "https://github.com/ioskn/mldl_htwg/blob/master/uebungen/dl_cnn_faces_net.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) Training using transfer learning\n",
    "In transfer learning you use an existing trained model with given weights trained on a different task. You then add a few layers yourself and only train them. The model you add should look like:\n",
    "\n",
    "```\n",
    "my_dense1 (Dense)            (None, 512)               262656    \n",
    "_________________________________________________________________\n",
    "my_dense2 (Dense)            (None, 512)               262656    \n",
    "_________________________________________________________________\n",
    "my_dense3 (Dense)            (None, 100)               51300     \n",
    "_________________________________________________________________\n",
    "my_dense4 (Dense)            (None, 8)                 808       \n",
    "```\n",
    "\n",
    "Use the following code as a starting point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "58892288/58889256 [==============================] - 27s 0us/step\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "img_height = img_width = 48\n",
    "base_model = VGG16(weights='imagenet', include_top=False,input_shape = (img_width, img_height, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer conv2d_2 is incompatible with the layer: : expected min_ndim=4, found ndim=2. Full shape received: (None, 512)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-f6477e4a101e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#Your code here\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#...\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mfc1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mConv2D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m411\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m48\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m48\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"same\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfc1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfc1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m#...\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    949\u001b[0m     \u001b[1;31m# >> model = tf.keras.Model(inputs, outputs)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    950\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_in_functional_construction_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 951\u001b[1;33m       return self._functional_construction_call(inputs, args, kwargs,\n\u001b[0m\u001b[0;32m    952\u001b[0m                                                 input_list)\n\u001b[0;32m    953\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m_functional_construction_call\u001b[1;34m(self, inputs, args, kwargs, input_list)\u001b[0m\n\u001b[0;32m   1088\u001b[0m           layer=self, inputs=inputs, build_graph=True, training=training_value):\n\u001b[0;32m   1089\u001b[0m         \u001b[1;31m# Check input assumptions set after layer building, e.g. input shape.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1090\u001b[1;33m         outputs = self._keras_tensor_symbolic_call(\n\u001b[0m\u001b[0;32m   1091\u001b[0m             inputs, input_masks, args, kwargs)\n\u001b[0;32m   1092\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m_keras_tensor_symbolic_call\u001b[1;34m(self, inputs, input_masks, args, kwargs)\u001b[0m\n\u001b[0;32m    820\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeras_tensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mKerasTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_signature\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 822\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_infer_output_signature\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    823\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    824\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_infer_output_signature\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m_infer_output_signature\u001b[1;34m(self, inputs, args, kwargs, input_masks)\u001b[0m\n\u001b[0;32m    860\u001b[0m           \u001b[1;31m# overridden).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m           \u001b[1;31m# TODO(kaftan): do we maybe_build here, or have we already done it?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 862\u001b[1;33m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_build\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    863\u001b[0m           \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    864\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m_maybe_build\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2682\u001b[0m     \u001b[1;31m# Check input assumptions set before layer building, e.g. input rank.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2683\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2684\u001b[1;33m       input_spec.assert_input_compatibility(\n\u001b[0m\u001b[0;32m   2685\u001b[0m           self.input_spec, inputs, self.name)\n\u001b[0;32m   2686\u001b[0m       \u001b[0minput_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    232\u001b[0m       \u001b[0mndim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrank\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mndim\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mndim\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mspec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin_ndim\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 234\u001b[1;33m         raise ValueError('Input ' + str(input_index) + ' of layer ' +\n\u001b[0m\u001b[0;32m    235\u001b[0m                          \u001b[0mlayer_name\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' is incompatible with the layer: '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m                          \u001b[1;34m': expected min_ndim='\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin_ndim\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input 0 of layer conv2d_2 is incompatible with the layer: : expected min_ndim=4, found ndim=2. Full shape received: (None, 512)"
     ]
    }
   ],
   "source": [
    "fc1 = base_model.output\n",
    "fc1 = Flatten()(fc1)\n",
    "#...\n",
    "#Your code here\n",
    "#...\n",
    "fc1 = Conv2D(411, kernel_size=(3, 3),activation='relu',input_shape=(48,48,15),padding=\"same\")(fc1)\n",
    "print(fc1)\n",
    "#...\n",
    "#Your code here\n",
    "#...\n",
    "fc1 = Dense(8,activation='softmax', name='my_dense4')(fc1)\n",
    "model_trans = tf.compat.v1.keras.Model(base_model.input, fc1) #Current hack for TF 2.0 and keras\n",
    "#model_trans.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my_dense4\n"
     ]
    }
   ],
   "source": [
    "#base_model.summary()\n",
    "for layer in model_trans.layers:\n",
    "  name = layer.name\n",
    "  if name.startswith('my'):\n",
    "    layer.trainable = True\n",
    "    print(name)\n",
    "  else:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the same analysis as in a) above. You should get an accuracy of approx 0.61"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c) Try to get better.\n",
    "\n",
    "Ideas: use dropout and other tricks to prevent overfitting. Try to learn some convulutional layers as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
